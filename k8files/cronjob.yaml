apiVersion: batch/v1beta1
kind: CronJob
metadata:
  labels:
    name: spark-submit
  name: spark-submit
  namespace: default
spec:
  concurrencyPolicy: Replace
  failedJobsHistoryLimit: 1
  jobTemplate:
    metadata:
      creationTimestamp: null
    spec:
      activeDeadlineSeconds: 900
      template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
          - name: spark-submit
            image: icpauloledo/spark-submit:v0.1
            imagePullPolicy: Always
            name: spark-submit
            env:
            - name: IMAGE_TAG
              value: latest
            - name: PROJECT_NAME
              value: batch-processor
            - name: CLASS_NAME
              value: batch.BatchProcessor
            - name: TAG_VERSION
              value: "0.1"
            - name: K8S_MASTER
              value: k8s://https://api.my-k8-cluster:443
            - name: APP_NAME
              value: batch-processor
            - name: IMAGE_URL
              value: my-docker-registry/batch-processor
            - name: ANALYTIC_URL
              value: jdbc:postgresql://my-postgresql:5432/analytics_datalake
            - name: DB_USERNAME
              value: my-db-user-name
            - name: DB_PASSWORD
              value: my-db-password
            - name: DB2_URL
              value: jdbc:postgresql://my-db:5432/my-db-url
            - name: DB2_USERNAME
              value: my-user-db2
            - name: DB2_PASSWORD
              value: my-db2-password
            - name: SPARK_EXECUTOR_INSTANCES
              value: "1"
            - name: SPARK_K8S_DRIVER_CORE_LIMIT
              value: "1000m"
            - name: SPARK_K8S_EXECUTOR_LIMIT_CORES
              value: "1000m"
            - name: SPARK_K8S_EXECUTOR_REQUEST_CORES
              value: "1000m"
            command: ["spark-submit"]
            args:
            -  --master=$(K8S_MASTER)
            -  --deploy-mode=cluster
            -  --name=$(APP_NAME)
            -  --class=$(CLASS_NAME)
            -  --conf=spark.executor.instances=$(SPARK_EXECUTOR_INSTANCES)
            -  --conf=spark.kubernetes.driver.limit.cores=$(SPARK_K8S_DRIVER_CORE_LIMIT)
            -  --conf=spark.kubernetes.executor.request.cores=$(SPARK_K8S_EXECUTOR_REQUEST_CORES)
            -  --conf=spark.kubernetes.executor.limit.cores=$(SPARK_K8S_EXECUTOR_LIMIT_CORES)
            -  --conf=spark.kubernetes.container.image=$(IMAGE_URL):$(IMAGE_TAG)
            -  --conf=spark.kubernetes.driverEnv.ANALYTIC_URL=$(ANALYTIC_URL)
            -  --conf=spark.kubernetes.driverEnv.ANALYTIC_USERNAME=$(ANALYTIC_USERNAME)
            -  --conf=spark.kubernetes.driverEnv.ANALYTIC_PASSWORD=$(ANALYTIC_PASSWORD)
            -  --conf=spark.kubernetes.driverEnv.LANDING_URL=$(LANDING_URL)
            -  --conf=spark.kubernetes.driverEnv.LANDING_USERNAME=$(LANDING_USERNAME)
            -  --conf=spark.kubernetes.driverEnv.LANDING_PASSWORD=$(LANDING_PASSWORD)
            -  --conf=spark.kubernetes.authenticate.driver.serviceAccountName=spark-submit   <<< here you should use yout kubernetes serviceAccount
            -  --executor-memory=2G
            -  --driver-memory=2G
            -  local:///$(PROJECT_NAME)/target/scala-2.11/$(PROJECT_NAME)-assembly-$(TAG_VERSION).jar
            resources:
              limits:
                cpu: 1000m
              requests:
                cpu: 1000m
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          nodeSelector:
            nodeNamespace: default
          restartPolicy: Never
          schedulerName: default-scheduler
          securityContext: {}
          terminationGracePeriodSeconds: 30
  schedule: 15 13 * * *
  successfulJobsHistoryLimit: 0
  suspend: false
