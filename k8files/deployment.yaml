apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: spark-submit
  labels:
    name: spark-submit
    namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      name: spark-submit
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        name: spark-submit
        type: dedicated
    spec:
      nodeSelector:
        nodeNamespace: default
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: type
                operator: In
                values:
                - dedicated
            topologyKey: kubernetes.io/hostname
      containers:
      - name: spark-submit
        image: icpauloledo/spark-submit:v0.1
        resources:
          requests:
            cpu: 1000m
          limits:
            cpu: 2000m

        env:
          - name: EXECUTOR_MEMORY
            value: 1G
          - name: DRIVER_MEMORY
            value: 1G
          - name: SPARK_IMAGE
            value: icpauloledo/spark:v0.1
          - name: K8_SERVICE_ACCOUNT
            value: spark-submit
          - name: CLASS_NAME
            value: org.apache.spark.examples.SparkPi
          - name: K8S_MASTER
            value: k8s://https://api.k8s.my-k8.tech:443
          - name: APP_NAME
            value: spark-pi
          - name: AP_ARTIFACT
            value: opt/spark/examples/jars/spark-examples_2.11-2.4.0.jar
          - name: SPARK_EXECUTOR_INSTANCES
            value: "1"
          - name: SPARK_K8S_DRIVER_CORE_LIMIT
            value: "1000m"
          - name: SPARK_K8S_EXECUTOR_LIMIT_CORES
            value: "1000m"
          - name: SPARK_K8S_EXECUTOR_REQUEST_CORES
            value: "1000m"

        command: ["spark-submit"]
        args:
        -  --master=$(K8S_MASTER)
        -  --deploy-mode=cluster 
        -  --name=$(APP_NAME)
        -  --class=$(CLASS_NAME)
        -  --conf=spark.executor.instances=$(SPARK_EXECUTOR_INSTANCES)
        -  --conf=spark.kubernetes.driver.limit.cores=$(SPARK_K8S_DRIVER_CORE_LIMIT)
        -  --conf=spark.kubernetes.executor.request.cores=$(SPARK_K8S_EXECUTOR_REQUEST_CORES)
        -  --conf=spark.kubernetes.executor.limit.cores=$(SPARK_K8S_EXECUTOR_LIMIT_CORES)
        -  --conf=spark.kubernetes.container.image=$(SPARK_IMAGE)
        -  --conf=spark.kubernetes.authenticate.driver.serviceAccountName=$(K8_SERVICE_ACCOUNT)
        -  --executor-memory=$(EXECUTOR_MEMORY)
        -  --driver-memory=$(DRIVER_MEMORY)
        -  local:///$(AP_ARTIFACT)
        # The service account name we created with the creating_service-account.sh. With that the pod will be able to
        # start the driver and exetor pods
        #-  --conf=spark.kubernetes.authenticate.driver.serviceAccountName=spark-submit
        
        # If you are using a dedicated instance group. I think It's a good idea when you are using K8s cronjobs
        #-  --conf=spark.kubernetes.node.selector.nodeNamespace=analytics-spark-submit
